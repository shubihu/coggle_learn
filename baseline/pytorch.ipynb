{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyMiecUkkHntWtjlxdE5brs4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDR0IelYwWOz","executionInfo":{"status":"ok","timestamp":1637907967678,"user_tz":-480,"elapsed":25600,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"b9550c0a-9631-4f2e-d688-db51ccaad08d"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XT38A8NDwfnA","executionInfo":{"status":"ok","timestamp":1637655548314,"user_tz":-480,"elapsed":608,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"6b61baa1-bfed-40d4-d3d2-54db830e616f"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["gdrive\tsample_data\n"]}]},{"cell_type":"code","metadata":{"id":"zfLy633Qw47D","executionInfo":{"status":"ok","timestamp":1637907974098,"user_tz":-480,"elapsed":340,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}}},"source":["import os\n","os.chdir(\"/content/gdrive/MyDrive/coggle_learn/learn\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJ8BgD6SxB6F"},"source":["!mkdir model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lb7shxSYxDMA","executionInfo":{"status":"ok","timestamp":1637907985831,"user_tz":-480,"elapsed":7005,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}}},"source":["import scipy.io as sio\n","import pandas as pd\n","import numpy as np\n","#from gensim.models import word2vec\n","from sklearn import preprocessing\n","from sklearn import metrics\n","import os\n","from tqdm import tqdm\n","from sklearn.model_selection import StratifiedKFold\n","import torch\n","from torch.utils.data import DataLoader\n","from torch import nn, optim\n","from torch.autograd import Variable\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import random\n","def setup_seed(seed):\n","     torch.manual_seed(seed)\n","     torch.cuda.manual_seed_all(seed)\n","     np.random.seed(seed)\n","     random.seed(seed)\n","     torch.backends.cudnn.deterministic = True\n","# 设置随机数种子\n","setup_seed(2021)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9nmZsqoxN2a","executionInfo":{"status":"ok","timestamp":1637907985833,"user_tz":-480,"elapsed":8,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}}},"source":["path = './'\n","df = pd.read_csv(path+'trainreference.csv')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndyUQnNvxY7y","executionInfo":{"status":"ok","timestamp":1637907985834,"user_tz":-480,"elapsed":7,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}}},"source":["from torch.utils.data import Dataset, DataLoader\n","setup_seed(2021)\n","class myDataset(Dataset):\n","    def __init__(self, df, idx=None, if_train=True):\n","        self.if_train = if_train\n","        if self.if_train:\n","            self.paths = df.loc[idx, 'name'].reset_index(drop=True)\n","            self.labels = df.loc[idx, 'tag'].reset_index(drop=True)\n","        else:\n","            self.paths = df['name'].reset_index(drop=True)\n","            self.labels = df['tag'].reset_index(drop=True)\n","\n","    def __getitem__(self, index):\n","        if self.if_train:\n","            sample = sio.loadmat(path+'train/'+self.paths[index])['ecgdata']\n","        else:\n","            sample = sio.loadmat(path+'val/'+self.paths[index])['ecgdata']\n","        return sample, self.labels[index]\n","\n","    def __len__(self):\n","        return len(self.paths)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0KEXOrIxc4r","executionInfo":{"status":"ok","timestamp":1637907994230,"user_tz":-480,"elapsed":364,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SeqNet(nn.Module):\n","    def __init__(self):\n","        super(SeqNet, self).__init__()\n","        # input \n","        self.conv1 = nn.Conv1d(12, 10, 50)\n","        self.conv2 = nn.Conv1d(12, 10, 200)\n","        self.conv3 = nn.Conv1d(12, 10, 500)\n","        self.conv4 = nn.Conv1d(12, 10, 1000)\n","        self.pooling = nn.MaxPool2d((1, 200))\n","        self.fc1 = nn.Linear(900, 64)\n","        self.fc2 = nn.Linear(64, 1)\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        \n","        out1 = self.pooling(F.relu(self.conv1(x)))\n","        out2 = self.pooling(F.relu(self.conv2(x)))\n","        out3 = self.pooling(F.relu(self.conv3(x)))\n","        out4 = self.pooling(F.relu(self.conv4(x)))\n","\n","        out = torch.cat([out1, out2, out3, out4], 2)\n","        out = out.view(batch_size, -1)\n","        out = self.fc1(out)\n","        out = F.relu(out)\n","        # out = F.dropout(out, p=0.2)\n","        out = self.fc2(out)\n","        return out"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQ1z7eDlxkdI","executionInfo":{"status":"ok","timestamp":1637908004320,"user_tz":-480,"elapsed":497,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}}},"source":["trainloader = torch.utils.data.DataLoader(\n","        myDataset(df, [i for i in range(0, 100)]), \n","        batch_size=30, shuffle=True, pin_memory=True, num_workers=1)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oM0VLT_O0iDR","executionInfo":{"status":"ok","timestamp":1637908030184,"user_tz":-480,"elapsed":22097,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"e8b6ff5d-1cbd-4ef5-9112-86d43caab6c2"},"source":["for i in trainloader:\n","    x = i[0]\n","    y = i[1]\n","    break\n","type(x)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Ct9nUrp7Vdh","executionInfo":{"status":"ok","timestamp":1637908088265,"user_tz":-480,"elapsed":325,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"9e496808-3cdb-41e9-e68a-07c3a01c97cc"},"source":["x.shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([30, 12, 5000])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"kDeCW4_h7hh3","executionInfo":{"status":"ok","timestamp":1637908165672,"user_tz":-480,"elapsed":317,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}}},"source":["conv1 = nn.Conv1d(12, 10, 50)\n","conv2 = nn.Conv1d(12, 10, 200)\n","conv3 = nn.Conv1d(12, 10, 500)\n","conv4 = nn.Conv1d(12, 10, 1000)\n","pooling = nn.MaxPool2d((1, 200))\n","fc1 = nn.Linear(900, 64)\n","fc2 = nn.Linear(64, 1)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nNfv4j-H7rb9","executionInfo":{"status":"ok","timestamp":1637908728159,"user_tz":-480,"elapsed":354,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"bf7ad33f-e210-438d-9ac4-03ad211022e3"},"source":["print('数据shape:', x.shape)\n","conv1 = nn.Conv1d(12, 10, 50)\n","pooling = nn.MaxPool2d((1,200))\n","print(pooling)\n","m = F.relu(conv1(x))\n","print(m.shape)\n","pool_out = pooling(m)\n","pool_out.shape"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["数据shape: torch.Size([30, 12, 5000])\n","MaxPool2d(kernel_size=(1, 200), stride=(1, 200), padding=0, dilation=1, ceil_mode=False)\n","torch.Size([30, 10, 4951])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([30, 10, 24])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HkFNgwsp_AjM","executionInfo":{"status":"ok","timestamp":1637909036910,"user_tz":-480,"elapsed":390,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"ec7f57f0-2299-4693-b12c-924f75baffc8"},"source":[" m = nn.MaxPool2d(3, stride=2)\n","# pool of non-square window\n","m = nn.MaxPool2d((3, 2), stride=(2, 1))\n","input = torch.randn(20, 16, 50, 32)\n","output = m(input)\n","output.shape"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 16, 24, 31])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5fu2mpw1W7_","executionInfo":{"status":"ok","timestamp":1637908037203,"user_tz":-480,"elapsed":3319,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"56433043-3911-4220-e901-e99da31a0687"},"source":["model = SeqNet()\n","\n","print(model(x))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0957],\n","        [ 0.0948],\n","        [ 0.0925],\n","        [ 0.1222],\n","        [ 0.0994],\n","        [ 0.0951],\n","        [ 0.0951],\n","        [ 0.1369],\n","        [ 0.0655],\n","        [ 0.1299],\n","        [ 0.1154],\n","        [ 0.1094],\n","        [ 0.1008],\n","        [ 0.1027],\n","        [ 0.0954],\n","        [ 0.0892],\n","        [ 0.1026],\n","        [ 0.1097],\n","        [-0.0969],\n","        [ 0.1188],\n","        [ 0.0831],\n","        [ 0.1217],\n","        [ 0.0923],\n","        [ 0.1131],\n","        [ 0.1148],\n","        [ 0.1390],\n","        [ 0.0688],\n","        [ 0.1194],\n","        [ 0.1042],\n","        [ 0.0961]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","metadata":{"id":"X64bUxTS11A8"},"source":["import time\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","max_epoch = 20\n","model_save_dir = path+'model/'\n","def train_model(model, criterion, optimizer, lr_scheduler=None):\n","    total_iters=len(trainloader)\n","    print('--------------total_iters:{}'.format(total_iters))\n","    since = time.time()\n","    best_loss = 1e7\n","    best_epoch = 0\n","    best_f1 = 0\n","    #\n","    iters = len(trainloader)\n","    for epoch in range(1,max_epoch+1):\n","        model.train(True)\n","        begin_time=time.time()\n","        # print('learning rate:{}'.format(optimizer.param_groups[-1]['lr']))\n","        print('Fold{} Epoch {}/{}'.format(fold+1,epoch, max_epoch))\n","        running_corrects_linear = 0\n","        count=0\n","        train_loss = []\n","        for i, (inputs, labels) in (enumerate(trainloader)):\n","            # print(inputs)\n","            count+=1\n","            inputs = inputs.to(device)\n","            labels = labels.float().to(device)\n","\n","            out_linear = model(inputs).to(device)\n","            loss = criterion(out_linear, labels.unsqueeze(1))\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            # 更新cosine学习率\n","            if lr_scheduler!=None:\n","                lr_scheduler.step(epoch + count / iters)\n","            if print_interval>0 and (i % print_interval == 0 or out_linear.size()[0] < train_batch_size):\n","                spend_time = time.time() - begin_time\n","                print(\n","                    ' Fold:{} Epoch:{}({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n","                        fold+1,epoch, count, total_iters,\n","                        loss.item(), optimizer.param_groups[-1]['lr'],\n","                        spend_time / count * total_iters // 60 - spend_time // 60))\n","            #\n","            train_loss.append(loss.item())\n","        #lr_scheduler.step()\n","        val_f1, val_loss= val_model(model, criterion)\n","        print('valf1: {:.4f}  valLogLoss: {:.4f}'.format(val_f1, val_loss))\n","        model_out_path = model_save_dir+\"/\"+'fold_'+str(fold+1)+'_'+str(epoch) + '.pth'\n","        best_model_out_path = model_save_dir+\"/\"+'fold_'+str(fold+1)+'_best'+'.pth'\n","        #save the best model\n","        if val_f1 >= best_f1:\n","            best_loss = val_loss\n","            best_f1 = val_f1\n","            best_epoch=epoch\n","            torch.save(model.state_dict(), best_model_out_path)\n","            print(\"save best epoch: {} best f1: {:.5f} best logloss: {:.5f}\".format(best_epoch,val_f1,val_loss))\n","  \n","    print('Fold{} Best f1: {:.3f} Best logloss: {:.3f} Best epoch:{}'.format(fold+1,best_f1, best_loss,best_epoch))\n","    time_elapsed = time.time() - since\n","    return best_loss, best_f1\n","\n","@torch.no_grad()\n","def val_model(model, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    cont = 0\n","    outPre = []\n","    outLabel = []\n","    pres_list=[]\n","    labels_list=[]\n","    for data in val_loader:\n","        inputs, labels = data\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","        outputs = model(inputs)\n","        pres_list+=outputs.sigmoid().detach().cpu().numpy().tolist()\n","        labels_list+=labels.detach().cpu().numpy().tolist()\n","\n","    preds = np.array(pres_list)\n","    labels = np.array(labels_list)\n","    val_f1 = metrics.f1_score(labels, list(map(lambda x: 1 if x > 0.5 else 0, preds)))\n","    log_loss = metrics.log_loss(labels, preds)#\n","    return val_f1, log_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSVi6w-85hdp","executionInfo":{"status":"ok","timestamp":1637659174228,"user_tz":-480,"elapsed":3175986,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"e6314863-9d83-42db-8bf6-ba7f09b23080"},"source":["setup_seed(2021)\n","skf = StratifiedKFold(n_splits=10)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","criterion = nn.BCEWithLogitsLoss()\n","print_interval=-1\n","kfold_best_loss = []\n","kfold_best_f1 = []\n","# print(len(df))\n","for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['tag'].values)):\n","    trainloader = torch.utils.data.DataLoader(\n","        myDataset(df, train_idx), \n","        batch_size=32, shuffle=True, pin_memory=True, num_workers=1)\n","    val_loader = torch.utils.data.DataLoader(\n","        myDataset(df, val_idx), \n","        batch_size=128, shuffle=False, pin_memory=True, num_workers=1)\n","    model = SeqNet()\n","    model.to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4 ,weight_decay=5e-4)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epoch)\n","\n","    best_loss, best_acc = train_model(model, criterion, optimizer, lr_scheduler=scheduler)\n","    kfold_best_loss.append(best_loss)\n","    kfold_best_f1.append(best_acc)\n","\n","print(kfold_best_f1)                  \n","print('loss...', np.mean(kfold_best_loss), 'f1...', np.mean(kfold_best_f1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","--------------total_iters:45\n","Fold1 Epoch 1/20\n","valf1: 0.6579  valLogLoss: 0.6611\n","save best epoch: 1 best f1: 0.65789 best logloss: 0.66112\n","Fold1 Epoch 2/20\n","valf1: 0.7361  valLogLoss: 0.5947\n","save best epoch: 2 best f1: 0.73611 best logloss: 0.59467\n","Fold1 Epoch 3/20\n","valf1: 0.7083  valLogLoss: 0.4831\n","Fold1 Epoch 4/20\n","valf1: 0.7639  valLogLoss: 0.4511\n","save best epoch: 4 best f1: 0.76389 best logloss: 0.45106\n","Fold1 Epoch 5/20\n","valf1: 0.8054  valLogLoss: 0.4288\n","save best epoch: 5 best f1: 0.80537 best logloss: 0.42876\n","Fold1 Epoch 6/20\n","valf1: 0.8075  valLogLoss: 0.4211\n","save best epoch: 6 best f1: 0.80745 best logloss: 0.42114\n","Fold1 Epoch 7/20\n","valf1: 0.7867  valLogLoss: 0.4153\n","Fold1 Epoch 8/20\n","valf1: 0.8000  valLogLoss: 0.4114\n","Fold1 Epoch 9/20\n","valf1: 0.8121  valLogLoss: 0.4157\n","save best epoch: 9 best f1: 0.81212 best logloss: 0.41567\n","Fold1 Epoch 10/20\n","valf1: 0.8054  valLogLoss: 0.3863\n","Fold1 Epoch 11/20\n","valf1: 0.8105  valLogLoss: 0.3839\n","Fold1 Epoch 12/20\n","valf1: 0.7838  valLogLoss: 0.3921\n","Fold1 Epoch 13/20\n","valf1: 0.8052  valLogLoss: 0.3878\n","Fold1 Epoch 14/20\n","valf1: 0.8105  valLogLoss: 0.3832\n","Fold1 Epoch 15/20\n","valf1: 0.8105  valLogLoss: 0.3793\n","Fold1 Epoch 16/20\n","valf1: 0.8258  valLogLoss: 0.3791\n","save best epoch: 16 best f1: 0.82581 best logloss: 0.37915\n","Fold1 Epoch 17/20\n","valf1: 0.8182  valLogLoss: 0.3793\n","Fold1 Epoch 18/20\n","valf1: 0.8182  valLogLoss: 0.3791\n","Fold1 Epoch 19/20\n","valf1: 0.8182  valLogLoss: 0.3791\n","Fold1 Epoch 20/20\n","valf1: 0.8182  valLogLoss: 0.3791\n","Fold1 Best f1: 0.826 Best logloss: 0.379 Best epoch:16\n","--------------total_iters:45\n","Fold2 Epoch 1/20\n","valf1: 0.6443  valLogLoss: 0.6694\n","save best epoch: 1 best f1: 0.64430 best logloss: 0.66937\n","Fold2 Epoch 2/20\n","valf1: 0.7853  valLogLoss: 0.5980\n","save best epoch: 2 best f1: 0.78528 best logloss: 0.59798\n","Fold2 Epoch 3/20\n","valf1: 0.8000  valLogLoss: 0.4878\n","save best epoch: 3 best f1: 0.80000 best logloss: 0.48779\n","Fold2 Epoch 4/20\n","valf1: 0.8121  valLogLoss: 0.4481\n","save best epoch: 4 best f1: 0.81212 best logloss: 0.44806\n","Fold2 Epoch 5/20\n","valf1: 0.8645  valLogLoss: 0.3970\n","save best epoch: 5 best f1: 0.86452 best logloss: 0.39699\n","Fold2 Epoch 6/20\n","valf1: 0.8623  valLogLoss: 0.3890\n","Fold2 Epoch 7/20\n","valf1: 0.8667  valLogLoss: 0.3738\n","save best epoch: 7 best f1: 0.86667 best logloss: 0.37376\n","Fold2 Epoch 8/20\n","valf1: 0.8662  valLogLoss: 0.3410\n","Fold2 Epoch 9/20\n","valf1: 0.8696  valLogLoss: 0.3527\n","save best epoch: 9 best f1: 0.86957 best logloss: 0.35273\n","Fold2 Epoch 10/20\n","valf1: 0.8795  valLogLoss: 0.3505\n","save best epoch: 10 best f1: 0.87952 best logloss: 0.35050\n","Fold2 Epoch 11/20\n","valf1: 0.8642  valLogLoss: 0.3433\n","Fold2 Epoch 12/20\n","valf1: 0.8625  valLogLoss: 0.3313\n","Fold2 Epoch 13/20\n","valf1: 0.8645  valLogLoss: 0.3268\n","Fold2 Epoch 14/20\n","valf1: 0.8718  valLogLoss: 0.3200\n","Fold2 Epoch 15/20\n","valf1: 0.8608  valLogLoss: 0.3234\n","Fold2 Epoch 16/20\n","valf1: 0.8608  valLogLoss: 0.3218\n","Fold2 Epoch 17/20\n","valf1: 0.8590  valLogLoss: 0.3181\n","Fold2 Epoch 18/20\n","valf1: 0.8590  valLogLoss: 0.3201\n","Fold2 Epoch 19/20\n","valf1: 0.8662  valLogLoss: 0.3198\n","Fold2 Epoch 20/20\n","valf1: 0.8662  valLogLoss: 0.3197\n","Fold2 Best f1: 0.880 Best logloss: 0.351 Best epoch:10\n","--------------total_iters:45\n","Fold3 Epoch 1/20\n","valf1: 0.5379  valLogLoss: 0.6886\n","save best epoch: 1 best f1: 0.53793 best logloss: 0.68860\n","Fold3 Epoch 2/20\n","valf1: 0.6715  valLogLoss: 0.6630\n","save best epoch: 2 best f1: 0.67153 best logloss: 0.66302\n","Fold3 Epoch 3/20\n","valf1: 0.7518  valLogLoss: 0.6404\n","save best epoch: 3 best f1: 0.75177 best logloss: 0.64044\n","Fold3 Epoch 4/20\n","valf1: 0.7632  valLogLoss: 0.6637\n","save best epoch: 4 best f1: 0.76316 best logloss: 0.66372\n","Fold3 Epoch 5/20\n","valf1: 0.8158  valLogLoss: 0.6772\n","save best epoch: 5 best f1: 0.81579 best logloss: 0.67716\n","Fold3 Epoch 6/20\n","valf1: 0.7862  valLogLoss: 0.6589\n","Fold3 Epoch 7/20\n","valf1: 0.7755  valLogLoss: 0.6310\n","Fold3 Epoch 8/20\n","valf1: 0.8105  valLogLoss: 0.6852\n","Fold3 Epoch 9/20\n","valf1: 0.8133  valLogLoss: 0.6471\n","Fold3 Epoch 10/20\n","valf1: 0.8105  valLogLoss: 0.6674\n","Fold3 Epoch 11/20\n","valf1: 0.8079  valLogLoss: 0.6337\n","Fold3 Epoch 12/20\n","valf1: 0.8312  valLogLoss: 0.6536\n","save best epoch: 12 best f1: 0.83117 best logloss: 0.65359\n","Fold3 Epoch 13/20\n","valf1: 0.8312  valLogLoss: 0.6507\n","save best epoch: 13 best f1: 0.83117 best logloss: 0.65073\n","Fold3 Epoch 14/20\n","valf1: 0.8258  valLogLoss: 0.6530\n","Fold3 Epoch 15/20\n","valf1: 0.8182  valLogLoss: 0.6504\n","Fold3 Epoch 16/20\n","valf1: 0.8258  valLogLoss: 0.6503\n","Fold3 Epoch 17/20\n","valf1: 0.8333  valLogLoss: 0.6485\n","save best epoch: 17 best f1: 0.83333 best logloss: 0.64846\n","Fold3 Epoch 18/20\n","valf1: 0.8387  valLogLoss: 0.6477\n","save best epoch: 18 best f1: 0.83871 best logloss: 0.64771\n","Fold3 Epoch 19/20\n","valf1: 0.8387  valLogLoss: 0.6476\n","save best epoch: 19 best f1: 0.83871 best logloss: 0.64765\n","Fold3 Epoch 20/20\n","valf1: 0.8387  valLogLoss: 0.6476\n","save best epoch: 20 best f1: 0.83871 best logloss: 0.64758\n","Fold3 Best f1: 0.839 Best logloss: 0.648 Best epoch:20\n","--------------total_iters:45\n","Fold4 Epoch 1/20\n","valf1: 0.6071  valLogLoss: 0.6730\n","save best epoch: 1 best f1: 0.60714 best logloss: 0.67302\n","Fold4 Epoch 2/20\n","valf1: 0.6883  valLogLoss: 0.6265\n","save best epoch: 2 best f1: 0.68831 best logloss: 0.62650\n","Fold4 Epoch 3/20\n","valf1: 0.7389  valLogLoss: 0.5704\n","save best epoch: 3 best f1: 0.73885 best logloss: 0.57041\n","Fold4 Epoch 4/20\n","valf1: 0.7389  valLogLoss: 0.5187\n","save best epoch: 4 best f1: 0.73885 best logloss: 0.51865\n","Fold4 Epoch 5/20\n","valf1: 0.7108  valLogLoss: 0.5092\n","Fold4 Epoch 6/20\n","valf1: 0.7403  valLogLoss: 0.5108\n","save best epoch: 6 best f1: 0.74026 best logloss: 0.51078\n","Fold4 Epoch 7/20\n","valf1: 0.7248  valLogLoss: 0.5055\n","Fold4 Epoch 8/20\n","valf1: 0.7383  valLogLoss: 0.5053\n","Fold4 Epoch 9/20\n","valf1: 0.7484  valLogLoss: 0.4895\n","save best epoch: 9 best f1: 0.74839 best logloss: 0.48947\n","Fold4 Epoch 10/20\n","valf1: 0.7643  valLogLoss: 0.4678\n","save best epoch: 10 best f1: 0.76433 best logloss: 0.46776\n","Fold4 Epoch 11/20\n","valf1: 0.7333  valLogLoss: 0.4810\n","Fold4 Epoch 12/20\n","valf1: 0.7595  valLogLoss: 0.4669\n","Fold4 Epoch 13/20\n","valf1: 0.7582  valLogLoss: 0.4663\n","Fold4 Epoch 14/20\n","valf1: 0.7515  valLogLoss: 0.4603\n","Fold4 Epoch 15/20\n","valf1: 0.7564  valLogLoss: 0.4570\n","Fold4 Epoch 16/20\n","valf1: 0.7613  valLogLoss: 0.4563\n","Fold4 Epoch 17/20\n","valf1: 0.7613  valLogLoss: 0.4556\n","Fold4 Epoch 18/20\n","valf1: 0.7613  valLogLoss: 0.4555\n","Fold4 Epoch 19/20\n","valf1: 0.7613  valLogLoss: 0.4555\n","Fold4 Epoch 20/20\n","valf1: 0.7613  valLogLoss: 0.4554\n","Fold4 Best f1: 0.764 Best logloss: 0.468 Best epoch:10\n","--------------total_iters:45\n","Fold5 Epoch 1/20\n","valf1: 0.5960  valLogLoss: 0.6695\n","save best epoch: 1 best f1: 0.59603 best logloss: 0.66951\n","Fold5 Epoch 2/20\n","valf1: 0.7013  valLogLoss: 0.6126\n","save best epoch: 2 best f1: 0.70130 best logloss: 0.61264\n","Fold5 Epoch 3/20\n","valf1: 0.8046  valLogLoss: 0.5592\n","save best epoch: 3 best f1: 0.80460 best logloss: 0.55924\n","Fold5 Epoch 4/20\n","valf1: 0.7730  valLogLoss: 0.5126\n","Fold5 Epoch 5/20\n","valf1: 0.7654  valLogLoss: 0.5195\n","Fold5 Epoch 6/20\n","valf1: 0.7805  valLogLoss: 0.4941\n","Fold5 Epoch 7/20\n","valf1: 0.8166  valLogLoss: 0.4753\n","save best epoch: 7 best f1: 0.81657 best logloss: 0.47533\n","Fold5 Epoch 8/20\n","valf1: 0.7952  valLogLoss: 0.4784\n","Fold5 Epoch 9/20\n","valf1: 0.7949  valLogLoss: 0.4620\n","Fold5 Epoch 10/20\n","valf1: 0.8121  valLogLoss: 0.4692\n","Fold5 Epoch 11/20\n","valf1: 0.8000  valLogLoss: 0.4653\n","Fold5 Epoch 12/20\n","valf1: 0.8000  valLogLoss: 0.4761\n","Fold5 Epoch 13/20\n","valf1: 0.8024  valLogLoss: 0.4698\n","Fold5 Epoch 14/20\n","valf1: 0.8000  valLogLoss: 0.4615\n","Fold5 Epoch 15/20\n","valf1: 0.7975  valLogLoss: 0.4612\n","Fold5 Epoch 16/20\n","valf1: 0.8049  valLogLoss: 0.4578\n","Fold5 Epoch 17/20\n","valf1: 0.7975  valLogLoss: 0.4551\n","Fold5 Epoch 18/20\n","valf1: 0.7975  valLogLoss: 0.4559\n","Fold5 Epoch 19/20\n","valf1: 0.7975  valLogLoss: 0.4558\n","Fold5 Epoch 20/20\n","valf1: 0.7975  valLogLoss: 0.4556\n","Fold5 Best f1: 0.817 Best logloss: 0.475 Best epoch:7\n","--------------total_iters:45\n","Fold6 Epoch 1/20\n","valf1: 0.3810  valLogLoss: 0.6737\n","save best epoch: 1 best f1: 0.38095 best logloss: 0.67367\n","Fold6 Epoch 2/20\n","valf1: 0.7630  valLogLoss: 0.5914\n","save best epoch: 2 best f1: 0.76301 best logloss: 0.59139\n","Fold6 Epoch 3/20\n","valf1: 0.7595  valLogLoss: 0.5145\n","Fold6 Epoch 4/20\n","valf1: 0.7848  valLogLoss: 0.4788\n","save best epoch: 4 best f1: 0.78481 best logloss: 0.47880\n","Fold6 Epoch 5/20\n","valf1: 0.7755  valLogLoss: 0.4305\n","Fold6 Epoch 6/20\n","valf1: 0.8105  valLogLoss: 0.4516\n","save best epoch: 6 best f1: 0.81046 best logloss: 0.45157\n","Fold6 Epoch 7/20\n","valf1: 0.8400  valLogLoss: 0.4048\n","save best epoch: 7 best f1: 0.84000 best logloss: 0.40482\n","Fold6 Epoch 8/20\n","valf1: 0.8537  valLogLoss: 0.3948\n","save best epoch: 8 best f1: 0.85366 best logloss: 0.39484\n","Fold6 Epoch 9/20\n","valf1: 0.8383  valLogLoss: 0.3951\n","Fold6 Epoch 10/20\n","valf1: 0.8537  valLogLoss: 0.3772\n","save best epoch: 10 best f1: 0.85366 best logloss: 0.37725\n","Fold6 Epoch 11/20\n","valf1: 0.8537  valLogLoss: 0.3816\n","save best epoch: 11 best f1: 0.85366 best logloss: 0.38161\n","Fold6 Epoch 12/20\n","valf1: 0.8500  valLogLoss: 0.3682\n","Fold6 Epoch 13/20\n","valf1: 0.8679  valLogLoss: 0.3630\n","save best epoch: 13 best f1: 0.86792 best logloss: 0.36302\n","Fold6 Epoch 14/20\n","valf1: 0.8553  valLogLoss: 0.3634\n","Fold6 Epoch 15/20\n","valf1: 0.8625  valLogLoss: 0.3636\n","Fold6 Epoch 16/20\n","valf1: 0.8679  valLogLoss: 0.3613\n","save best epoch: 16 best f1: 0.86792 best logloss: 0.36129\n","Fold6 Epoch 17/20\n","valf1: 0.8571  valLogLoss: 0.3622\n","Fold6 Epoch 18/20\n","valf1: 0.8571  valLogLoss: 0.3619\n","Fold6 Epoch 19/20\n","valf1: 0.8571  valLogLoss: 0.3619\n","Fold6 Epoch 20/20\n","valf1: 0.8571  valLogLoss: 0.3619\n","Fold6 Best f1: 0.868 Best logloss: 0.361 Best epoch:16\n","--------------total_iters:45\n","Fold7 Epoch 1/20\n","valf1: 0.6709  valLogLoss: 0.6717\n","save best epoch: 1 best f1: 0.67089 best logloss: 0.67170\n","Fold7 Epoch 2/20\n","valf1: 0.7702  valLogLoss: 0.6238\n","save best epoch: 2 best f1: 0.77019 best logloss: 0.62377\n","Fold7 Epoch 3/20\n","valf1: 0.7848  valLogLoss: 0.4910\n","save best epoch: 3 best f1: 0.78481 best logloss: 0.49099\n","Fold7 Epoch 4/20\n","valf1: 0.8098  valLogLoss: 0.4968\n","save best epoch: 4 best f1: 0.80982 best logloss: 0.49682\n","Fold7 Epoch 5/20\n","valf1: 0.8158  valLogLoss: 0.4778\n","save best epoch: 5 best f1: 0.81579 best logloss: 0.47777\n","Fold7 Epoch 6/20\n","valf1: 0.8267  valLogLoss: 0.4420\n","save best epoch: 6 best f1: 0.82667 best logloss: 0.44196\n","Fold7 Epoch 7/20\n","valf1: 0.8153  valLogLoss: 0.4838\n","Fold7 Epoch 8/20\n","valf1: 0.8212  valLogLoss: 0.4506\n","Fold7 Epoch 9/20\n","valf1: 0.8235  valLogLoss: 0.4701\n","Fold7 Epoch 10/20\n","valf1: 0.8105  valLogLoss: 0.4456\n","Fold7 Epoch 11/20\n","valf1: 0.8077  valLogLoss: 0.4475\n","Fold7 Epoch 12/20\n","valf1: 0.8133  valLogLoss: 0.4518\n","Fold7 Epoch 13/20\n","valf1: 0.8052  valLogLoss: 0.4385\n","Fold7 Epoch 14/20\n","valf1: 0.8258  valLogLoss: 0.4378\n","Fold7 Epoch 15/20\n","valf1: 0.8158  valLogLoss: 0.4364\n","Fold7 Epoch 16/20\n","valf1: 0.8158  valLogLoss: 0.4350\n","Fold7 Epoch 17/20\n","valf1: 0.8158  valLogLoss: 0.4356\n","Fold7 Epoch 18/20\n","valf1: 0.8158  valLogLoss: 0.4350\n","Fold7 Epoch 19/20\n","valf1: 0.8158  valLogLoss: 0.4351\n","Fold7 Epoch 20/20\n","valf1: 0.8158  valLogLoss: 0.4350\n","Fold7 Best f1: 0.827 Best logloss: 0.442 Best epoch:6\n","--------------total_iters:45\n","Fold8 Epoch 1/20\n","valf1: 0.5635  valLogLoss: 0.6873\n","save best epoch: 1 best f1: 0.56354 best logloss: 0.68732\n","Fold8 Epoch 2/20\n","valf1: 0.7467  valLogLoss: 0.6434\n","save best epoch: 2 best f1: 0.74667 best logloss: 0.64343\n","Fold8 Epoch 3/20\n","valf1: 0.8025  valLogLoss: 0.5220\n","save best epoch: 3 best f1: 0.80247 best logloss: 0.52202\n","Fold8 Epoch 4/20\n","valf1: 0.8344  valLogLoss: 0.4457\n","save best epoch: 4 best f1: 0.83444 best logloss: 0.44573\n","Fold8 Epoch 5/20\n","valf1: 0.8364  valLogLoss: 0.4877\n","save best epoch: 5 best f1: 0.83636 best logloss: 0.48766\n","Fold8 Epoch 6/20\n","valf1: 0.8344  valLogLoss: 0.4581\n","Fold8 Epoch 7/20\n","valf1: 0.8366  valLogLoss: 0.4451\n","save best epoch: 7 best f1: 0.83660 best logloss: 0.44508\n","Fold8 Epoch 8/20\n","valf1: 0.8387  valLogLoss: 0.4208\n","save best epoch: 8 best f1: 0.83871 best logloss: 0.42084\n","Fold8 Epoch 9/20\n","valf1: 0.8481  valLogLoss: 0.4051\n","save best epoch: 9 best f1: 0.84810 best logloss: 0.40510\n","Fold8 Epoch 10/20\n","valf1: 0.8462  valLogLoss: 0.4223\n","Fold8 Epoch 11/20\n","valf1: 0.8428  valLogLoss: 0.4177\n","Fold8 Epoch 12/20\n","valf1: 0.8590  valLogLoss: 0.4109\n","save best epoch: 12 best f1: 0.85897 best logloss: 0.41094\n","Fold8 Epoch 13/20\n","valf1: 0.8608  valLogLoss: 0.4142\n","save best epoch: 13 best f1: 0.86076 best logloss: 0.41424\n","Fold8 Epoch 14/20\n","valf1: 0.8535  valLogLoss: 0.4053\n","Fold8 Epoch 15/20\n","valf1: 0.8590  valLogLoss: 0.4106\n","Fold8 Epoch 16/20\n","valf1: 0.8590  valLogLoss: 0.4074\n","Fold8 Epoch 17/20\n","valf1: 0.8535  valLogLoss: 0.4075\n","Fold8 Epoch 18/20\n","valf1: 0.8590  valLogLoss: 0.4074\n","Fold8 Epoch 19/20\n","valf1: 0.8590  valLogLoss: 0.4072\n","Fold8 Epoch 20/20\n","valf1: 0.8590  valLogLoss: 0.4072\n","Fold8 Best f1: 0.861 Best logloss: 0.414 Best epoch:13\n","--------------total_iters:45\n","Fold9 Epoch 1/20\n","valf1: 0.5248  valLogLoss: 0.6832\n","save best epoch: 1 best f1: 0.52482 best logloss: 0.68319\n","Fold9 Epoch 2/20\n","valf1: 0.7553  valLogLoss: 0.6280\n","save best epoch: 2 best f1: 0.75532 best logloss: 0.62799\n","Fold9 Epoch 3/20\n","valf1: 0.7101  valLogLoss: 0.5571\n","Fold9 Epoch 4/20\n","valf1: 0.7338  valLogLoss: 0.5312\n","Fold9 Epoch 5/20\n","valf1: 0.7919  valLogLoss: 0.4470\n","save best epoch: 5 best f1: 0.79195 best logloss: 0.44701\n","Fold9 Epoch 6/20\n","valf1: 0.8000  valLogLoss: 0.4530\n","save best epoch: 6 best f1: 0.80000 best logloss: 0.45299\n","Fold9 Epoch 7/20\n","valf1: 0.7848  valLogLoss: 0.4208\n","Fold9 Epoch 8/20\n","valf1: 0.8027  valLogLoss: 0.4539\n","save best epoch: 8 best f1: 0.80272 best logloss: 0.45391\n","Fold9 Epoch 9/20\n","valf1: 0.7862  valLogLoss: 0.4122\n","Fold9 Epoch 10/20\n","valf1: 0.7832  valLogLoss: 0.4259\n","Fold9 Epoch 11/20\n","valf1: 0.7801  valLogLoss: 0.4224\n","Fold9 Epoch 12/20\n","valf1: 0.7891  valLogLoss: 0.4093\n","Fold9 Epoch 13/20\n","valf1: 0.7862  valLogLoss: 0.4004\n","Fold9 Epoch 14/20\n","valf1: 0.7808  valLogLoss: 0.4027\n","Fold9 Epoch 15/20\n","valf1: 0.7746  valLogLoss: 0.4080\n","Fold9 Epoch 16/20\n","valf1: 0.7917  valLogLoss: 0.4045\n","Fold9 Epoch 17/20\n","valf1: 0.7746  valLogLoss: 0.4044\n","Fold9 Epoch 18/20\n","valf1: 0.7917  valLogLoss: 0.4028\n","Fold9 Epoch 19/20\n","valf1: 0.7917  valLogLoss: 0.4028\n","Fold9 Epoch 20/20\n","valf1: 0.7917  valLogLoss: 0.4029\n","Fold9 Best f1: 0.803 Best logloss: 0.454 Best epoch:8\n","--------------total_iters:45\n","Fold10 Epoch 1/20\n","valf1: 0.6538  valLogLoss: 0.6877\n","save best epoch: 1 best f1: 0.65385 best logloss: 0.68770\n","Fold10 Epoch 2/20\n","valf1: 0.6711  valLogLoss: 0.6389\n","save best epoch: 2 best f1: 0.67105 best logloss: 0.63886\n","Fold10 Epoch 3/20\n","valf1: 0.7532  valLogLoss: 0.5412\n","save best epoch: 3 best f1: 0.75325 best logloss: 0.54119\n","Fold10 Epoch 4/20\n","valf1: 0.7612  valLogLoss: 0.5062\n","save best epoch: 4 best f1: 0.76119 best logloss: 0.50622\n","Fold10 Epoch 5/20\n","valf1: 0.8077  valLogLoss: 0.4541\n","save best epoch: 5 best f1: 0.80769 best logloss: 0.45412\n","Fold10 Epoch 6/20\n","valf1: 0.7445  valLogLoss: 0.4947\n","Fold10 Epoch 7/20\n","valf1: 0.8000  valLogLoss: 0.4395\n","Fold10 Epoch 8/20\n","valf1: 0.8163  valLogLoss: 0.4231\n","save best epoch: 8 best f1: 0.81633 best logloss: 0.42314\n","Fold10 Epoch 9/20\n","valf1: 0.8276  valLogLoss: 0.4085\n","save best epoch: 9 best f1: 0.82759 best logloss: 0.40851\n","Fold10 Epoch 10/20\n","valf1: 0.8414  valLogLoss: 0.4066\n","save best epoch: 10 best f1: 0.84138 best logloss: 0.40663\n","Fold10 Epoch 11/20\n","valf1: 0.8000  valLogLoss: 0.4104\n","Fold10 Epoch 12/20\n","valf1: 0.8276  valLogLoss: 0.4071\n","Fold10 Epoch 13/20\n","valf1: 0.8085  valLogLoss: 0.4081\n","Fold10 Epoch 14/20\n","valf1: 0.8252  valLogLoss: 0.4068\n","Fold10 Epoch 15/20\n","valf1: 0.8333  valLogLoss: 0.4031\n","Fold10 Epoch 16/20\n","valf1: 0.8085  valLogLoss: 0.4098\n","Fold10 Epoch 17/20\n","valf1: 0.8085  valLogLoss: 0.4079\n","Fold10 Epoch 18/20\n","valf1: 0.8085  valLogLoss: 0.4065\n","Fold10 Epoch 19/20\n","valf1: 0.8085  valLogLoss: 0.4066\n","Fold10 Epoch 20/20\n","valf1: 0.8085  valLogLoss: 0.4066\n","Fold10 Best f1: 0.841 Best logloss: 0.407 Best epoch:10\n","[0.8258064516129033, 0.8795180722891567, 0.8387096774193549, 0.7643312101910829, 0.8165680473372781, 0.8679245283018868, 0.8266666666666667, 0.8607594936708861, 0.8027210884353743, 0.8413793103448275]\n","loss... 0.43983253173234466 f1... 0.8324384546269418\n"]}]},{"cell_type":"code","metadata":{"id":"xlMoM5DC5m9M"},"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","def load_model(weight_path):\n","    print(weight_path)\n","    model = SeqNet()\n","    model.load_state_dict(torch.load(weight_path))\n","    model.to(device)\n","    model.eval()\n","    return model\n","\n","@torch.no_grad()\n","def predict(test_loader):\n","    ret = 0\n","    for i, model in enumerate(model_list):\n","        print('----model ', i)\n","        pres_list = []\n","        for data in tqdm(test_loader):\n","            inputs, _a = data\n","            inputs = inputs.cuda()\n","            outputs = model(inputs)\n","            pres_list+=outputs.sigmoid().detach().cpu().numpy().tolist()\n","        ret += np.array(pres_list) / len(model_list)\n","    return list(map(lambda x: 1 if x > 0.5 else 0, ret))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EaAU5EjU7sHP","executionInfo":{"status":"ok","timestamp":1637722593601,"user_tz":-480,"elapsed":59875,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"660b6bc6-50ea-4b67-f047-6acf0a446076"},"source":["device=torch.device('cuda')\n","model_list=[]\n","for i in range(10):\n","    model_list.append(load_model(path+'model/fold_'+str(i+1)+'_best.pth'))\n","import os\n","\n","sub = pd.read_csv(path+'answer.csv')\n","test_loader = torch.utils.data.DataLoader(\n","        myDataset(sub, if_train=False), \n","        batch_size=64, shuffle=False, num_workers=16, pin_memory=True)\n","sub['tag'] = predict(test_loader)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./model/fold_1_best.pth\n","./model/fold_2_best.pth\n","./model/fold_3_best.pth\n","./model/fold_4_best.pth\n","./model/fold_5_best.pth\n","./model/fold_6_best.pth\n","./model/fold_7_best.pth\n","./model/fold_8_best.pth\n","./model/fold_9_best.pth\n","./model/fold_10_best.pth\n","----model  0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:18<00:00,  2.67s/it]\n"]},{"output_type":"stream","name":"stdout","text":["----model  1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:02<00:00,  2.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:02<00:00,  2.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  6\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  7\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:02<00:00,  2.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:02<00:00,  2.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["----model  9\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:02<00:00,  2.43it/s]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_W-c63y4OoI","executionInfo":{"status":"ok","timestamp":1637722738488,"user_tz":-480,"elapsed":332,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"9a4bf5b4-6ce5-41ab-d9e0-ffab0ae93eb1"},"source":["# sub.head()\n","!ls answer"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sub_20211118_0.83244.csv\n"]}]},{"cell_type":"code","metadata":{"id":"nxn8CfDJ7x6N"},"source":["# !mkdir answer\n","sub.to_csv(path+'answer/answer_1124.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLcVkWcxGfgm","executionInfo":{"status":"ok","timestamp":1637660147458,"user_tz":-480,"elapsed":1708,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"4bc0dbf0-0793-461e-a457-6d1c670b2ee2"},"source":["!zip model.zip model/*"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: model/fold_10_best.pth (deflated 8%)\n","  adding: model/fold_1_best.pth (deflated 8%)\n","  adding: model/fold_2_best.pth (deflated 8%)\n","  adding: model/fold_3_best.pth (deflated 8%)\n","  adding: model/fold_4_best.pth (deflated 8%)\n","  adding: model/fold_5_best.pth (deflated 8%)\n","  adding: model/fold_6_best.pth (deflated 8%)\n","  adding: model/fold_7_best.pth (deflated 8%)\n","  adding: model/fold_8_best.pth (deflated 8%)\n","  adding: model/fold_9_best.pth (deflated 8%)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xl56v5evI9IS","executionInfo":{"status":"ok","timestamp":1637659964459,"user_tz":-480,"elapsed":473,"user":{"displayName":"王家宾","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10291435559252023072"}},"outputId":"50afbca0-a058-4afc-c6ba-179ec38e4142"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["answer\t    model      train\t trainreference.csv  Untitled0.ipynb  val.7z\n","answer.csv  model.zip  train.7z  train.zip\t     val\n"]}]}]}